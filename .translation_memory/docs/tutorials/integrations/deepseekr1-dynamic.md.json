{
  "source_file_path_relative_to_docusaurus_root": "docs/tutorials/integrations/deepseekr1-dynamic.md",
  "source_file_content_hash": "da711c44a3c05533c5881bf70566814ce8118368813836365c7a86039db65619",
  "segments": [
    {
      "segment_id": "58cfcc64",
      "source_content": "---\nsidebar_position: 1\ntitle: \"ğŸ‹ Run DeepSeek R1 Dynamic 1.58-bit with Llama.cpp\"\n---",
      "source_content_hash": "4abfe4df8cfd5202f8c228a683877c596d7a3e60e697405bb85c7b5245c3fd40",
      "node_type": "yaml",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_58cfcc64"
      }
    },
    {
      "segment_id": "bb42506b",
      "source_content": "A huge shoutout to **UnslothAI** for their incredible efforts! Thanks to their hard work, we can now run the **full DeepSeek-R1** 671B parameter model in its dynamic 1.58-bit quantized form (compressed to just 131GB) on **Llama.cpp**! And the best part? You no longer have to despair about needing massive enterprise-class GPUs or servers â€” itâ€™s possible to run this model on your personal machine (albeit slowly for most consumer hardware).",
      "source_content_hash": "1c345b955431d1db4df1a5020590c4cf727e862bd3405de16c344f706ddd10cd",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "**UnslothAI**ã®ç´ æ™´ã‚‰ã—ã„åŠªåŠ›ã«å¿ƒã‹ã‚‰ã®æ„Ÿè¬ã‚’ï¼å½¼ã‚‰ã®å°½åŠ›ã«ã‚ˆã‚Šã€**å®Œå…¨ç‰ˆDeepSeek-R1** 671Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’å‹•çš„1.58ãƒ“ãƒƒãƒˆé‡å­åŒ–å½¢å¼ï¼ˆ131GBã«åœ§ç¸®ï¼‰ã§**Llama.cpp**ä¸Šã§å‹•ä½œã•ã›ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸï¼ãã—ã¦ä½•ã‚ˆã‚Šå¬‰ã—ã„ã®ã¯ã€å¤§è¦æ¨¡ãªã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚¯ãƒ©ã‚¹ã®GPUã‚„ã‚µãƒ¼ãƒãƒ¼ã‚’å¿…è¦ã¨ã›ãšã€å€‹äººã®ãƒã‚·ãƒ³ï¼ˆãŸã ã—ã»ã¨ã‚“ã©ã®ã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒãƒ¼ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§ã¯ä½é€Ÿã§ã™ãŒï¼‰ã§ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œå¯èƒ½ã«ãªã£ãŸã“ã¨ã§ã™ã€‚"
      }
    },
    {
      "segment_id": "a239c14b",
      "source_content": ":::note\nThe only true **DeepSeek-R1** model on Ollama is the **671B version** available here: [https://ollama.com/library/deepseek-r1:671b](https://ollama.com/library/deepseek-r1:671b). Other versions are **distilled** models.\n:::",
      "source_content_hash": "d76bf2ffdb247b28a6d7fb09b2afb617fc1501ced06538e2bc743c255f921315",
      "node_type": "containerDirective",
      "translatable": true,
      "translations": {
        "ja": ":::note\nOllamaã§åˆ©ç”¨å¯èƒ½ãªçœŸã®**DeepSeek-R1**ãƒ¢ãƒ‡ãƒ«ã¯ã€ä»¥ä¸‹ã®**671Bãƒãƒ¼ã‚¸ãƒ§ãƒ³**ã®ã¿ã§ã™: [https://ollama.com/library/deepseek-r1:671b](https://ollama.com/library/deepseek-r1:671b)ã€‚ä»–ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯**è’¸ç•™**ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\n:::"
      }
    },
    {
      "segment_id": "8d66f911",
      "source_content": "This guide focuses on running the **full DeepSeek-R1 Dynamic 1.58-bit quantized model** using **Llama.cpp** integrated with **Sage WebUI**. For this tutorial, weâ€™ll demonstrate the steps with an **M4 Max + 128GB RAM** machine. You can adapt the settings to your own configuration.",
      "source_content_hash": "6af0b427839bc9fe09c15e6f730d9751a5e8736845cc43440c47e2535f4981a0",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€**Llama.cpp**ã¨çµ±åˆã—ãŸ**Sage WebUI**ã‚’ä½¿ç”¨ã—ã¦**å®Œå…¨ç‰ˆDeepSeek-R1å‹•çš„1.58ãƒ“ãƒƒãƒˆé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«**ã‚’å®Ÿè¡Œã™ã‚‹æ‰‹é †ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚æœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯**M4 Max + 128GB RAM**ãƒã‚·ãƒ³ã§ã®è¨­å®šä¾‹ã‚’ç¤ºã—ã¾ã™ãŒã€å„è‡ªã®ç’°å¢ƒã«åˆã‚ã›ã¦èª¿æ•´å¯èƒ½ã§ã™ã€‚"
      }
    },
    {
      "segment_id": "1af1a865",
      "source_content": "---",
      "source_content_hash": "cb3f91d54eee30e53e35b2b99905f70f169ed549fd78909d3dac2defc9ed8d3b",
      "node_type": "thematicBreak",
      "translatable": true,
      "translations": {
        "ja": "---"
      }
    },
    {
      "segment_id": "cbe57f9d",
      "source_content": "## Step 1: Install Llama.cpp",
      "source_content_hash": "81dff59777bb32df8977e4ea8ca3cf7f18893b4bef3a29cd4a86fea33a1e2af0",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "## ã‚¹ãƒ†ãƒƒãƒ—1: Llama.cppã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
      }
    },
    {
      "segment_id": "39b3772c",
      "source_content": "You can either:",
      "source_content_hash": "b0798dc15b470db76d58928f0d8d12e717d8c267a9f8cfda23e2282a6bcdba35",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æ–¹æ³•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã¾ã™:"
      }
    },
    {
      "segment_id": "034de896",
      "source_content": "- [Download the prebuilt binaries](https://github.com/ggerganov/llama.cpp/releases)  \n- **Or build it yourself**: Follow the instructions here: [Llama.cpp Build Guide](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md)",
      "source_content_hash": "10d1ac2a8e7ebc662c9782114eb1806b5557484865610a9c5b55af65951f5f7f",
      "node_type": "list",
      "translatable": true,
      "translations": {
        "ja": "- [ãƒ—ãƒªãƒ“ãƒ«ãƒ‰æ¸ˆã¿ãƒã‚¤ãƒŠãƒªã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰](https://github.com/ggerganov/llama.cpp/releases)  \n- **ã¾ãŸã¯è‡ªåˆ†ã§ãƒ“ãƒ«ãƒ‰**: [Llama.cppãƒ“ãƒ«ãƒ‰ã‚¬ã‚¤ãƒ‰](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md)ã®æ‰‹é †ã«å¾“ã£ã¦ãã ã•ã„"
      }
    },
    {
      "segment_id": "63622bf0",
      "source_content": "## Step 2: Download the Model Provided by UnslothAI",
      "source_content_hash": "e3f1195af807f110dc5c372ec7321f68ac7b961fa45974f090fd2549fe31c41a",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "## ã‚¹ãƒ†ãƒƒãƒ—2: UnslothAIæä¾›ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
      }
    },
    {
      "segment_id": "98cfbd53",
      "source_content": "Head over to [Unslothâ€™s Hugging Face page](https://huggingface.co/unsloth/DeepSeek-R1-GGUF) and download the appropriate **dynamic quantized version** of DeepSeek-R1. For this tutorial, weâ€™ll use the **1.58-bit (131GB)** version, which is highly optimized yet remains surprisingly functional.",
      "source_content_hash": "2d649c864341cc7b0dcce422a40b355d00b1a2329af52463cc1bbbd7ef40ce7d",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "[Unslothã®Hugging Faceãƒšãƒ¼ã‚¸](https://huggingface.co/unsloth/DeepSeek-R1-GGUF)ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€é©åˆ‡ãª**å‹•çš„é‡å­åŒ–ãƒãƒ¼ã‚¸ãƒ§ãƒ³**ã®DeepSeek-R1ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚æœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€é«˜åº¦ã«æœ€é©åŒ–ã•ã‚ŒãªãŒã‚‰é©šãã»ã©æ©Ÿèƒ½æ€§ã‚’ä¿ã¤**1.58ãƒ“ãƒƒãƒˆç‰ˆï¼ˆ131GBï¼‰**ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
      }
    },
    {
      "segment_id": "6b3596a2",
      "source_content": ":::tip\nKnow your \"working directory\" â€” where your Python script or terminal session is running. The model files will download to a subfolder of that directory by default, so be sure you know its path! For example, if you're running the command below in `/Users/yourname/Documents/projects`, your downloaded model will be saved under `/Users/yourname/Documents/projects/DeepSeek-R1-GGUF`. \n:::",
      "source_content_hash": "2ead8d93453ef3c9b5cfbb953ea6dfdc6b3f559349d7d95e328f6fa238968d6e",
      "node_type": "containerDirective",
      "translatable": true,
      "translations": {
        "ja": ":::tip\nã€Œä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€ï¼ˆPythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚„ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã¦ã„ã‚‹å ´æ‰€ï¼‰ã‚’æŠŠæ¡ã—ã¦ãã ã•ã„ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãã®ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ãŸã‚ã€ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚ä¾‹ãˆã°`/Users/yourname/Documents/projects`ã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹å ´åˆã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯`/Users/yourname/Documents/projects/DeepSeek-R1-GGUF`ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚\n:::"
      }
    },
    {
      "segment_id": "dfe931a2",
      "source_content": "To understand more about UnslothAIâ€™s development process and why these dynamic quantized versions are so efficient, check out their blog post: [UnslothAI DeepSeek R1 Dynamic Quantization](https://unsloth.ai/blog/deepseekr1-dynamic).",
      "source_content_hash": "56f90110b28d6f261325de3a1a60fada01ab3a5b4e8950174ffcfa9aff2359b6",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "UnslothAIã®é–‹ç™ºãƒ—ãƒ­ã‚»ã‚¹ã¨ã€ãªãœã“ã‚Œã‚‰ã®å‹•çš„é‡å­åŒ–ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒã“ã‚Œã»ã©åŠ¹ç‡çš„ãªã®ã‹ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„å ´åˆã¯ã€å½¼ã‚‰ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’å‚ç…§ã—ã¦ãã ã•ã„: [UnslothAI DeepSeek R1 Dynamic Quantization](https://unsloth.ai/blog/deepseekr1-dynamic)ã€‚"
      }
    },
    {
      "segment_id": "4daf5cdd",
      "source_content": "Hereâ€™s how to download the model programmatically:",
      "source_content_hash": "711974190bbdfec14a9dd78e7103c8c478df2338bb1923a8e8105222395df86c",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–¹æ³•ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:"
      }
    },
    {
      "segment_id": "6543a19e",
      "source_content": "```python\n# Install Hugging Face dependencies before running this:\n# pip install huggingface_hub hf_transfer\n\nfrom huggingface_hub import snapshot_download\n\nsnapshot_download(\n    repo_id = \"unsloth/DeepSeek-R1-GGUF\",  # Specify the Hugging Face repo\n    local_dir = \"DeepSeek-R1-GGUF\",         # Model will download into this directory\n    allow_patterns = [\"*UD-IQ1_S*\"],        # Only download the 1.58-bit version\n)\n```",
      "source_content_hash": "45e3c5d04b2524fc5ff56bffaef901e1ebda4cad83f5c459b250f043acde9bbf",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_6543a19e"
      }
    },
    {
      "segment_id": "13d9cd70",
      "source_content": "Once the download completes, youâ€™ll find the model files in a directory structure like this:",
      "source_content_hash": "68b28609c22f234a1099213e3be33ea2346e0fd2d61c0dfe1f86da9e80a23d27",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã§è¦‹ã¤ã‹ã‚Šã¾ã™:"
      }
    },
    {
      "segment_id": "48ec8149",
      "source_content": "```\nDeepSeek-R1-GGUF/\nâ”œâ”€â”€ DeepSeek-R1-UD-IQ1_S/\nâ”‚   â”œâ”€â”€ DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf\nâ”‚   â”œâ”€â”€ DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf\nâ”‚   â”œâ”€â”€ DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf\n```",
      "source_content_hash": "cacdee8280f0e002400e8310362b0d541a0df112e88eb2b9f8796968de375cee",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_48ec8149"
      }
    },
    {
      "segment_id": "f90b9a6a",
      "source_content": ":::info\nğŸ› ï¸ Update paths in the later steps to **match your specific directory structure**. For example, if your script was in `/Users/tim/Downloads`, the full path to the GGUF file would be:  \n`/Users/tim/Downloads/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf`.\n:::",
      "source_content_hash": "2cb5a64b07652a910083458ec1920f50513d39f77c112416eb91104ac16bb5ac",
      "node_type": "containerDirective",
      "translatable": true,
      "translations": {
        "ja": ":::info\nğŸ› ï¸ å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã§ä½¿ç”¨ã™ã‚‹ãƒ‘ã‚¹ã¯**å®Ÿéš›ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã«åˆã‚ã›ã¦æ›´æ–°**ã—ã¦ãã ã•ã„ã€‚ä¾‹ãˆã°ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒ`/Users/tim/Downloads`ã«ã‚ã‚‹å ´åˆã€GGUFãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ•ãƒ«ãƒ‘ã‚¹ã¯:  \n`/Users/tim/Downloads/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf`ã¨ãªã‚Šã¾ã™ã€‚\n:::"
      }
    },
    {
      "segment_id": "e912d935",
      "source_content": "## Step 3: Make Sure Sage WebUI is Installed and Running",
      "source_content_hash": "e7f8380de73215f8283a71e17c1a2f40a61a0307b1470d714b0f13b2361b025d",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "## ã‚¹ãƒ†ãƒƒãƒ—3: Sage WebUIã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨èµ·å‹•ç¢ºèª"
      }
    },
    {
      "segment_id": "8b2a52b1",
      "source_content": "If you donâ€™t already have **Sage WebUI** installed, no worries! Itâ€™s a simple setup. Just follow the [Sage WebUI documentation here](/). Once installed, start the application â€” weâ€™ll connect it in a later step to interact with the DeepSeek-R1 model.",
      "source_content_hash": "79f8c853f6469632feb39defa26740d039bd1921295f22066782bfa65bc52490",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "**Sage WebUI**ãŒã¾ã ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã§ã‚‚å¿ƒé…ã‚ã‚Šã¾ã›ã‚“ï¼ç°¡å˜ã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ãã¾ã™ã€‚[Sage WebUIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](/ja)ã«å¾“ã£ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•ã—ã¦ãã ã•ã„ã€‚å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã§DeepSeek-R1ãƒ¢ãƒ‡ãƒ«ã¨æ¥ç¶šã—ã¾ã™ã€‚"
      }
    },
    {
      "segment_id": "8c6d7ee7",
      "source_content": "## Step 4: Serve the Model Using Llama.cpp",
      "source_content_hash": "11784486039387290c07e2e78a180b002bcd2bc1e6679798143c3fe31e0e26ca",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "## ã‚¹ãƒ†ãƒƒãƒ—4: Llama.cppã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒ¼ãƒãƒ¼ã¨ã—ã¦èµ·å‹•"
      }
    },
    {
      "segment_id": "06e24cc0",
      "source_content": "Now that the model is downloaded, the next step is to run it using **Llama.cppâ€™s server mode**. Before you begin:",
      "source_content_hash": "81416b55f135b67584ac697a067a413aa221f434dfd53e8f21ad12a4eb0ba3de",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ãŸã‚‰ã€æ¬¡ã¯**Llama.cppã®ã‚µãƒ¼ãƒãƒ¼ãƒ¢ãƒ¼ãƒ‰**ã§å®Ÿè¡Œã—ã¾ã™ã€‚é–‹å§‹å‰ã«ä»¥ä¸‹ã®ç‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:"
      }
    },
    {
      "segment_id": "af7d8a35",
      "source_content": "1. **Locate the `llama-server` binary.**  \n   If you built from source (as outlined in Step 1), the `llama-server` executable will be located in `llama.cpp/build/bin`. Navigate to this directory by using the `cd` command:  \n   ```bash\n   cd [path-to-llama-cpp]/llama.cpp/build/bin\n   ```\n\n   Replace `[path-to-llama-cpp]` with the location where you cloned or built Llama.cpp. For example:  \n   ```bash\n   cd ~/Documents/workspace/llama.cpp/build/bin\n   ```\n\n2. **Point to your model folder.**  \n   Use the full path to the downloaded GGUF files created in Step 2. When serving the model, specify the first part of the split GGUF files (e.g., `DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf`).",
      "source_content_hash": "528f1e7f7c1a7e1d2c3b7c8455ef4d1057f9e396a8b1ffd365419c455da58d00",
      "node_type": "list",
      "translatable": true,
      "translations": {
        "ja": "1. **`llama-server`ãƒã‚¤ãƒŠãƒªã®å ´æ‰€ã‚’ç¢ºèªã™ã‚‹**  \n   ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ“ãƒ«ãƒ‰ã—ãŸå ´åˆï¼ˆã‚¹ãƒ†ãƒƒãƒ—1ã§èª¬æ˜ã—ãŸé€šã‚Šï¼‰ã€`llama-server`å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã¯`llama.cpp/build/bin`ã«ã‚ã‚Šã¾ã™ã€‚`cd`ã‚³ãƒãƒ³ãƒ‰ã§ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã—ã¾ã™:  \n   ```bash\n   cd [path-to-llama-cpp]/llama.cpp/build/bin\n   ```\n\n   `[path-to-llama-cpp]`ã¯Llama.cppã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã¾ãŸã¯ãƒ“ãƒ«ãƒ‰ã—ãŸå ´æ‰€ã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚ä¾‹:  \n   ```bash\n   cd ~/Documents/workspace/llama.cpp/build/bin\n   ```\n\n2. **ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®šã™ã‚‹**  \n   ã‚¹ãƒ†ãƒƒãƒ—2ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸGGUFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒ¼ãƒãƒ¼ã§å®Ÿè¡Œã™ã‚‹éš›ã¯ã€åˆ†å‰²ã•ã‚ŒãŸGGUFãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€åˆã®éƒ¨åˆ†ã‚’æŒ‡å®šã—ã¾ã™ï¼ˆä¾‹: `DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf`ï¼‰ã€‚"
      }
    },
    {
      "segment_id": "a1fa5396",
      "source_content": "Hereâ€™s the command to start the server:",
      "source_content_hash": "ddb7235d2291a90f1e39ddc15e9d698e81619cec9aee83d1d1a131884ddf1efa",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã™ã‚‹ã‚³ãƒãƒ³ãƒ‰ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:"
      }
    },
    {
      "segment_id": "65400624",
      "source_content": "```bash\n./llama-server \\\n    --model /[your-directory]/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n    --port 10000 \\\n    --ctx-size 1024 \\\n    --n-gpu-layers 40\n```",
      "source_content_hash": "d9caeedf7f8a8ace13c15cb26d7e2841bb9f598f2b1d2f2c16eec9c7da573323",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_65400624"
      }
    },
    {
      "segment_id": "f1e357f9",
      "source_content": ":::tip\nğŸ”‘ **Parameters to Customize Based on Your Machine:**  \n\n- **`--model`:** Replace `/[your-directory]/` with the path where the GGUF files were downloaded in Step 2.  \n- **`--port`:** The server default is `8080`, but feel free to change it based on your port availability.  \n- **`--ctx-size`:** Determines context length (number of tokens). You can increase it if your hardware allows, but be cautious of rising RAM/VRAM usage.  \n- **`--n-gpu-layers`:** Set the number of layers you want to offload to your GPU for faster inference. The exact number depends on your GPUâ€™s memory capacity â€” reference Unslothâ€™s table for specific recommendations.\n:::",
      "source_content_hash": "5fcf2f6584d392ed83e74c270900509ddfca4e50f0336e3c8456044cbe6443b8",
      "node_type": "containerDirective",
      "translatable": true,
      "translations": {
        "ja": ":::tip\nğŸ”‘ **ãƒã‚·ãƒ³ã«å¿œã˜ã¦ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**  \n\n- **`--model`:** GGUFãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‘ã‚¹ã«`/[your-directory]/`ã‚’ç½®ãæ›ãˆã¾ã™ï¼ˆã‚¹ãƒ†ãƒƒãƒ—2å‚ç…§ï¼‰ã€‚  \n- **`--port`:** ã‚µãƒ¼ãƒãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ¼ãƒˆã¯`8080`ã§ã™ãŒã€åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ãƒˆã«å¤‰æ›´å¯èƒ½ã§ã™ã€‚  \n- **`--ctx-size`:** ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰ã‚’æ±ºå®šã—ã¾ã™ã€‚ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãŒè¨±ã›ã°å¢—ã‚„ã›ã¾ã™ãŒã€RAM/VRAMä½¿ç”¨é‡ã®å¢—åŠ ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚  \n- **`--n-gpu-layers`:** GPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ã‚’è¨­å®šã—ã€æ¨è«–ã‚’é«˜é€ŸåŒ–ã—ã¾ã™ã€‚å…·ä½“çš„ãªæ•°ã¯GPUãƒ¡ãƒ¢ãƒªå®¹é‡ã«ä¾å­˜ã—ã¾ã™ â€” Unslothã®è¡¨ã‚’å‚ç…§ã—ã¦æ¨å¥¨å€¤ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n:::"
      }
    },
    {
      "segment_id": "d79a4831",
      "source_content": "For example, if your model was downloaded to `/Users/tim/Documents/workspace`, your command would look like this:",
      "source_content_hash": "82661c3cfb964e68cb230672b28bbebe1b94c1014972646874b3bac3ebcb7757",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "ä¾‹ãˆã°ã€ãƒ¢ãƒ‡ãƒ«ãŒ`/Users/tim/Documents/workspace`ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸå ´åˆã€ã‚³ãƒãƒ³ãƒ‰ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™:"
      }
    },
    {
      "segment_id": "1cf51900",
      "source_content": "```bash\n./llama-server \\\n    --model /Users/tim/Documents/workspace/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n    --port 10000 \\\n    --ctx-size 1024 \\\n    --n-gpu-layers 40\n```",
      "source_content_hash": "54ef646d96cb09f440ef97d6ef2c0cbefbf585a1069cfcb706427903eee362d9",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_1cf51900"
      }
    },
    {
      "segment_id": "0801d969",
      "source_content": "Once the server starts, it will host a **local OpenAI-compatible API** endpoint at:",
      "source_content_hash": "7f7e48dec2888444fc2c27bb458209a47beb76cb43cb03d5ef997e10473e2ba3",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®å ´æ‰€ã§**ãƒ­ãƒ¼ã‚«ãƒ«ã®OpenAIäº’æ›API**ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒãƒ›ã‚¹ãƒˆã•ã‚Œã¾ã™:"
      }
    },
    {
      "segment_id": "f39e109c",
      "source_content": "```\nhttp://127.0.0.1:10000\n```",
      "source_content_hash": "43e5096104c54fb52f93861c8fc024693d48dec7dcdb01c00ccf8d232b7d371e",
      "node_type": "code",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_f39e109c"
      }
    },
    {
      "segment_id": "379173ce",
      "source_content": ":::info\nğŸ–¥ï¸ **Llama.cpp Server Running**  \n\n![Server Screenshot](/images/tutorials/deepseek/serve.png)  \n\nAfter running the command, you should see a message confirming the server is active and listening on port 10000.\n:::",
      "source_content_hash": "954a008be0519c2ccfc24cc3bf0ff9210d7023ce14e5455fffd9f778a96f0e96",
      "node_type": "containerDirective",
      "translatable": true,
      "translations": {
        "ja": ":::info\nğŸ–¥ï¸ **Llama.cppã‚µãƒ¼ãƒãƒ¼ç¨¼åƒä¸­**  \n\n![ã‚µãƒ¼ãƒãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ](/images/tutorials/deepseek/serve.png)  \n\nã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œå¾Œã€ã‚µãƒ¼ãƒãƒ¼ãŒãƒãƒ¼ãƒˆ10000ã§ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã«å¾…æ©Ÿã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚\n:::"
      }
    },
    {
      "segment_id": "b6152880",
      "source_content": "Be sure to **keep this terminal session running**, as it serves the model for all subsequent steps.",
      "source_content_hash": "64e9b0e14dc73a57ab869100ddbc04bea17718801a19699d4f034a6e4904d021",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "**ã“ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯å®Ÿè¡Œã—ãŸã¾ã¾ã«ã—ã¦ãã ã•ã„**ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒä»¥é™ã®ã‚¹ãƒ†ãƒƒãƒ—ã§åˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã™ã€‚"
      }
    },
    {
      "segment_id": "23b96cc0",
      "source_content": "## Step 5: Connect Llama.cpp to Sage WebUI",
      "source_content_hash": "3f6d21267d4a8d948723d8690662b222b2d4823bea6cc91a9bf5e7338d67fdfc",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "## ã‚¹ãƒ†ãƒƒãƒ—5: Llama.cppã‚’Sage WebUIã«æ¥ç¶šã™ã‚‹"
      }
    },
    {
      "segment_id": "4b530389",
      "source_content": "1. Go to **Admin Settings** in Sage WebUI.  \n2. Navigate to **Connections > OpenAI Connections.**  \n3. Add the following details for the new connection:  \n   - URL: `http://127.0.0.1:10000/v1` (or `http://host.docker.internal:10000/v1` when running Sage WebUI in docker)\n   - API Key: `none`",
      "source_content_hash": "6c36a35a33f0941b35f24c36f80dd81083d47ca8c1d05819eb69cdc5f4cde000",
      "node_type": "list",
      "translatable": true,
      "translations": {
        "ja": "1. Sage WebUIã®**ç®¡ç†è€…è¨­å®š**ã«ç§»å‹•ã—ã¾ã™ã€‚  \n2. **æ¥ç¶š > OpenAIæ¥ç¶š**ã«é€²ã¿ã¾ã™ã€‚  \n3. æ–°ã—ã„æ¥ç¶šã«ä»¥ä¸‹ã®è©³ç´°ã‚’è¿½åŠ ã—ã¾ã™:  \n   - URL: `http://127.0.0.1:10000/v1`ï¼ˆDockerã§Sage WebUIã‚’å®Ÿè¡Œã—ã¦ã„ã‚‹å ´åˆã¯`http://host.docker.internal:10000/v1`ï¼‰\n   - APIã‚­ãƒ¼: `none`"
      }
    },
    {
      "segment_id": "1865603e",
      "source_content": ":::info\nğŸ–¥ï¸ **Adding Connection in Sage WebUI**  \n\n![Connection Screenshot](/images/tutorials/deepseek/connection.png)  \n\nAfter running the command, you should see a message confirming the server is active and listening on port 10000.\n:::",
      "source_content_hash": "6399ee0281a17d8c56f962f4b85ed7c6e629a6c56fd95c0781dcd00987c1db06",
      "node_type": "containerDirective",
      "translatable": true,
      "translations": {
        "ja": ":::info\nğŸ–¥ï¸ **Sage WebUIã§ã®æ¥ç¶šè¿½åŠ **  \n\n![æ¥ç¶šã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ](/images/tutorials/deepseek/connection.png)  \n\nã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œå¾Œã€ã‚µãƒ¼ãƒãƒ¼ãŒãƒãƒ¼ãƒˆ10000ã§ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã«å¾…æ©Ÿã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚\n:::"
      }
    },
    {
      "segment_id": "8664ed0b",
      "source_content": "Once the connection is saved, you can start querying **DeepSeek-R1** directly from Sage WebUI! ğŸ‰",
      "source_content_hash": "2b3d4d5c4faa34326e98e34fcb612d546f2bea6811a4c105727354eabccfd900",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "æ¥ç¶šã‚’ä¿å­˜ã™ã‚‹ã¨ã€Sage WebUIã‹ã‚‰ç›´æ¥**DeepSeek-R1**ã«ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼ ğŸ‰"
      }
    },
    {
      "segment_id": "ce87f480",
      "source_content": "---",
      "source_content_hash": "cb3f91d54eee30e53e35b2b99905f70f169ed549fd78909d3dac2defc9ed8d3b",
      "node_type": "thematicBreak",
      "translatable": true,
      "translations": {
        "ja": "---"
      }
    },
    {
      "segment_id": "a0d0e29e",
      "source_content": "## Example: Generating Responses",
      "source_content_hash": "0924e17f07ed36a62783d6de52dd9a49175900fe17f4576a532d6bfb5dad3edb",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "## ä¾‹: å¿œç­”ã®ç”Ÿæˆ"
      }
    },
    {
      "segment_id": "be17ccc3",
      "source_content": "You can now use Sage WebUIâ€™s chat interface to interact with the **DeepSeek-R1 Dynamic 1.58-bit model**.",
      "source_content_hash": "13fe77e10d0c24e96a04ac7d24ac8435232c394d9b21e335cfa4d7399a4454d9",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "ã“ã‚Œã§ã€Sage WebUIã®ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ã¦**DeepSeek-R1 Dynamic 1.58-bitãƒ¢ãƒ‡ãƒ«**ã¨å¯¾è©±ã§ãã¾ã™ã€‚"
      }
    },
    {
      "segment_id": "ad3e22c6",
      "source_content": "![Response Screenshot](/images/tutorials/deepseek/response.png)",
      "source_content_hash": "069028836d183be509eba6d78a81f70f89a7bf3cc28cb3d3bfcb66a8a4c00ba3",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "![å¿œç­”ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ](/images/tutorials/deepseek/response.png)"
      }
    },
    {
      "segment_id": "d4b679d6",
      "source_content": "---",
      "source_content_hash": "cb3f91d54eee30e53e35b2b99905f70f169ed549fd78909d3dac2defc9ed8d3b",
      "node_type": "thematicBreak",
      "translatable": true,
      "translations": {
        "ja": "---"
      }
    },
    {
      "segment_id": "55dc794c",
      "source_content": "## Notes and Considerations",
      "source_content_hash": "9e224546c490ffbbb394e6f938e677fc512597ae75dda786e74ff4202dc8ee09",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "## æ³¨æ„ç‚¹ã¨è€ƒæ…®äº‹é …"
      }
    },
    {
      "segment_id": "e99d73d3",
      "source_content": "- **Performance:**  \n  Running a massive 131GB model like DeepSeek-R1 on personal hardware will be **slow**. Even with our M4 Max (128GB RAM), inference speeds were modest. But the fact that it works at all is a testament to UnslothAIâ€™s optimizations.  \n\n- **VRAM/Memory Requirements:**  \n  Ensure sufficient VRAM and system RAM for optimal performance. With low-end GPUs or CPU-only setups, expect slower speeds (but itâ€™s still doable!).",
      "source_content_hash": "f78e888b79f4a4565e5456f65f1f48162e583e158bab2fb86b6b50d19fd789ad",
      "node_type": "list",
      "translatable": true,
      "translations": {
        "ja": "- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:**  \n  DeepSeek-R1ã®ã‚ˆã†ãª131GBã‚‚ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’å€‹äººã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§å®Ÿè¡Œã™ã‚‹ã¨ã€**éå¸¸ã«é…ããªã‚Šã¾ã™**ã€‚M4 Maxï¼ˆ128GB RAMï¼‰ã®ã‚ˆã†ãªé«˜æ€§èƒ½ãƒã‚·ãƒ³ã§ã‚‚ã€æ¨è«–é€Ÿåº¦ã¯æ§ãˆã‚ã§ã™ã€‚ã—ã‹ã—ã€ãã‚Œã§ã‚‚å‹•ä½œã™ã‚‹ã¨ã„ã†äº‹å®Ÿã¯ã€UnslothAIã®æœ€é©åŒ–ã®è³œç‰©ã§ã™ã€‚  \n\n- **VRAM/ãƒ¡ãƒ¢ãƒªè¦ä»¶:**  \n  æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ã«ã¯ã€ååˆ†ãªVRAMã¨ã‚·ã‚¹ãƒ†ãƒ RAMã‚’ç¢ºä¿ã—ã¦ãã ã•ã„ã€‚ãƒ­ãƒ¼ã‚¨ãƒ³ãƒ‰GPUã‚„CPUã®ã¿ã®ç’°å¢ƒã§ã¯ã€é€Ÿåº¦ãŒã•ã‚‰ã«ä½ä¸‹ã—ã¾ã™ï¼ˆãã‚Œã§ã‚‚å®Ÿè¡Œå¯èƒ½ã§ã™ï¼ï¼‰ã€‚"
      }
    },
    {
      "segment_id": "57180eb4",
      "source_content": "---",
      "source_content_hash": "cb3f91d54eee30e53e35b2b99905f70f169ed549fd78909d3dac2defc9ed8d3b",
      "node_type": "thematicBreak",
      "translatable": true,
      "translations": {
        "ja": "---"
      }
    },
    {
      "segment_id": "2dbfc5f7",
      "source_content": "Thanks to **UnslothAI** and **Llama.cpp**, running one of the largest open-source reasoning models, **DeepSeek-R1** (1.58-bit version), is finally accessible to individuals. While itâ€™s challenging to run such models on consumer hardware, the ability to do so without massive computational infrastructure is a significant technological milestone.",
      "source_content_hash": "2f5b980bce4700204bf7469a75ebb5f8b2d5e6da7e160dd9cb35294040210828",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "**UnslothAI**ã¨**Llama.cpp**ã®ãŠã‹ã’ã§ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®å¤§è¦æ¨¡æ¨è«–ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹**DeepSeek-R1**ï¼ˆ1.58ãƒ“ãƒƒãƒˆç‰ˆï¼‰ã‚’å€‹äººã§ã‚‚å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚ã“ã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒãƒ¼ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§å®Ÿè¡Œã™ã‚‹ã®ã¯å›°é›£ã§ã™ãŒã€å¤§è¦æ¨¡ãªè¨ˆç®—ã‚¤ãƒ³ãƒ•ãƒ©ãªã—ã§å®Ÿç¾ã§ãã‚‹ã“ã¨ã¯ã€æŠ€è¡“çš„ãªå¤§ããªé€²æ­©ã§ã™ã€‚"
      }
    },
    {
      "segment_id": "6b40e7a2",
      "source_content": "â­ Big thanks to the community for pushing the boundaries of open AI research.",
      "source_content_hash": "1b1dc99fae9859d5f2fc771a2e4b1a945e7f0283d1d2db0c8eec95b921318da2",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "â­ ã‚ªãƒ¼ãƒ—ãƒ³AIç ”ç©¶ã®é™ç•Œã‚’æŠ¼ã—åºƒã’ã‚‹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«æ„Ÿè¬ã—ã¾ã™ã€‚"
      }
    },
    {
      "segment_id": "39f476dc",
      "source_content": "Happy experimenting! ğŸš€",
      "source_content_hash": "bfdabdce53e113f440750c14b40c2d02e029fcd6389549b85fc838c486648ef3",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "æ¥½ã—ã„å®Ÿé¨“ã‚’ï¼ ğŸš€"
      }
    }
  ],
  "target_i18n_subpath": "docusaurus-plugin-content-docs/current/tutorials/integrations/deepseekr1-dynamic.md",
  "last_updated_timestamp": "2025-06-06T09:21:13.788169+00:00",
  "schema_version": "1.0",
  "translated_versions": {
    "ja": "da711c44a3c05533c5881bf70566814ce8118368813836365c7a86039db65619"
  }
}