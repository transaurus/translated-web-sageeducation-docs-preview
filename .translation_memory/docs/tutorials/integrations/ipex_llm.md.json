{
  "source_file_path_relative_to_docusaurus_root": "docs/tutorials/integrations/ipex_llm.md",
  "source_file_content_hash": "05a6adae94d46beeb9b2e32173a168103f2934541a47bac4bfe4f3da541b681a",
  "segments": [
    {
      "segment_id": "58cfcc64",
      "source_content": "---\nsidebar_position: 11\ntitle: \"ğŸ–¥ï¸ Local LLM Setup with IPEX-LLM on Intel GPU\"\n---",
      "source_content_hash": "dcd62d0bd2c8725980351a02caf8b68f4056dfbaf27c3c694fc9bdd6bd34f998",
      "node_type": "yaml",
      "translatable": false,
      "translations": {
        "ja": "@@untranslatable_placeholder_58cfcc64"
      }
    },
    {
      "segment_id": "0eeea6cc",
      "source_content": ":::warning\nThis tutorial is a community contribution and is not supported by the Sage WebUI team. It serves only as a demonstration on how to customize Sage WebUI for your specific use case. Want to contribute? Check out the contributing tutorial.\n:::",
      "source_content_hash": "9deffb738cd50d6595571ff813d2388bdda279aad6934a7d7fdfb239906531ed",
      "node_type": "containerDirective",
      "translatable": true,
      "translations": {
        "ja": ":::warning\nã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¯ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«ã‚ˆã‚‹è²¢çŒ®ã§ã‚ã‚Šã€Sage WebUIãƒãƒ¼ãƒ ã«ã‚ˆã‚‹ã‚µãƒãƒ¼ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ç‰¹å®šã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«åˆã‚ã›ã¦Sage WebUIã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹æ–¹æ³•ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã—ã¦æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚è²¢çŒ®ã—ãŸã„å ´åˆã¯ã€ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚¬ã‚¤ãƒ‰ã‚’ã”è¦§ãã ã•ã„ã€‚\n:::"
      }
    },
    {
      "segment_id": "6cae1da3",
      "source_content": ":::note\nThis guide is verified with Sage WebUI setup through [Manual Installation](/getting-started/index.md).\n:::",
      "source_content_hash": "177e7900fc2712607a9ec5ccd46d02dc3e35b61ee25fa03fe01ff1b1bea0cc36",
      "node_type": "containerDirective",
      "translatable": true,
      "translations": {
        "ja": ":::note\nã“ã®ã‚¬ã‚¤ãƒ‰ã¯ã€[æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](/getting-started/index.md)ã§ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ãŸSage WebUIã§æ¤œè¨¼ã•ã‚Œã¦ã„ã¾ã™ã€‚\n:::"
      }
    },
    {
      "segment_id": "70153c6e",
      "source_content": "# Local LLM Setup with IPEX-LLM on Intel GPU",
      "source_content_hash": "74dbbf6a92543c9ee1a1b867c5a2cf824d52089b1ce5e37f3db56c0c1f2e0e37",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "# Intel GPUã§ã®IPEX-LLMã‚’ç”¨ã„ãŸãƒ­ãƒ¼ã‚«ãƒ«LLMã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"
      }
    },
    {
      "segment_id": "976d532b",
      "source_content": ":::info\n[**IPEX-LLM**](https://github.com/intel-analytics/ipex-llm) is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc A-Series, Flex and Max) with very low latency.\n:::",
      "source_content_hash": "98ed4d5ee6b1fa6e6a880b078eba0497cc108fb537441b5490b541d3f32335db",
      "node_type": "containerDirective",
      "translatable": true,
      "translations": {
        "ja": ":::info\n[**IPEX-LLM**](https://github.com/intel-analytics/ipex-llm)ã¯ã€Intel CPUãŠã‚ˆã³GPUï¼ˆä¾‹ï¼šiGPUæ­è¼‰ã®ãƒ­ãƒ¼ã‚«ãƒ«PCã€Arc Aã‚·ãƒªãƒ¼ã‚ºã€Flexã€Maxãªã©ã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ¼ãƒˆGPUï¼‰ä¸Šã§LLMã‚’éå¸¸ã«ä½é…å»¶ã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã®PyTorchãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚\n:::"
      }
    },
    {
      "segment_id": "1acb9a05",
      "source_content": "This tutorial demonstrates how to setup Sage WebUI with **IPEX-LLM accelerated Ollama backend hosted on Intel GPU**. By following this guide, you will be able to setup Sage WebUI even on a low-cost PC (i.e. only with integrated GPU) with a smooth experience.",
      "source_content_hash": "89c61a3715d78ee0c23a5ad17ca2ff9b003df0d5ac65b6b1f74e2e0e0ee27c8b",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€**Intel GPUä¸Šã§ãƒ›ã‚¹ãƒˆã•ã‚ŒãŸIPEX-LLMåŠ é€ŸOllamaãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰**ã‚’ä½¿ç”¨ã—ã¦Sage WebUIã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚ã“ã®ã‚¬ã‚¤ãƒ‰ã«å¾“ã†ã“ã¨ã§ã€ä½ã‚³ã‚¹ãƒˆã®PCï¼ˆä¾‹ï¼šçµ±åˆGPUã®ã¿æ­è¼‰ï¼‰ã§ã‚‚ã‚¹ãƒ ãƒ¼ã‚ºãªä½“é¨“ã§Sage WebUIã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ãã¾ã™ã€‚"
      }
    },
    {
      "segment_id": "63622bf0",
      "source_content": "## Start Ollama Serve on Intel GPU",
      "source_content_hash": "0d595bb7eb08507f9a25e19a23b5d710e543cafec6794f52222e410360c36407",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "## Intel GPUã§ã®Ollamaã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•"
      }
    },
    {
      "segment_id": "98cfbd53",
      "source_content": "Refer to [this guide](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama_quickstart.html) from IPEX-LLM official documentation about how to install and run Ollama serve accelerated by IPEX-LLM on Intel GPU.",
      "source_content_hash": "58d1af9b818ec686054073ebbfb30d754b899fcf6cdae4f92dafd47c16666ddd",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "Intel GPUä¸Šã§IPEX-LLMã«ã‚ˆã£ã¦åŠ é€Ÿã•ã‚ŒãŸOllamaã‚µãƒ¼ãƒãƒ¼ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨å®Ÿè¡Œæ–¹æ³•ã«ã¤ã„ã¦ã¯ã€IPEX-LLMå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®[ã“ã®ã‚¬ã‚¤ãƒ‰](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama_quickstart.html)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"
      }
    },
    {
      "segment_id": "a2f1eaea",
      "source_content": ":::tip\nIf you would like to reach the Ollama service from another machine, make sure you set or export the environment variable `OLLAMA_HOST=0.0.0.0` before executing the command `ollama serve`.\n:::",
      "source_content_hash": "bfafa3c6031e1b3df106f6445fc863b59105cff849633093f507927181718009",
      "node_type": "containerDirective",
      "translatable": true,
      "translations": {
        "ja": ":::tip\nåˆ¥ã®ãƒã‚·ãƒ³ã‹ã‚‰Ollamaã‚µãƒ¼ãƒ“ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ãŸã„å ´åˆã¯ã€`ollama serve`ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹å‰ã«ç’°å¢ƒå¤‰æ•°`OLLAMA_HOST=0.0.0.0`ã‚’è¨­å®šã¾ãŸã¯ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„ã€‚\n:::"
      }
    },
    {
      "segment_id": "d06306f7",
      "source_content": "## Configure Sage WebUI",
      "source_content_hash": "19a3f49cf114d325a369ddd1c749e074fbde5779afd3b29dcb351e6a5052f113",
      "node_type": "heading",
      "translatable": true,
      "translations": {
        "ja": "## Sage WebUIã®è¨­å®š"
      }
    },
    {
      "segment_id": "73ab96a3",
      "source_content": "Access the Ollama settings through **Settings -> Connections** in the menu. By default, the **Ollama Base URL** is preset to https://localhost:11434, as illustrated in the snapshot below. To verify the status of the Ollama service connection, click the **Refresh button** located next to the textbox. If the WebUI is unable to establish a connection with the Ollama server, you will see an error message stating, `WebUI could not connect to Ollama`.",
      "source_content_hash": "9722b4c0b877c14f428c548d981bffd0ff78845fdcbe9c4c8931af20ec543148",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®**è¨­å®š -> æ¥ç¶š**ã‹ã‚‰Ollamaè¨­å®šã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€**Ollamaãƒ™ãƒ¼ã‚¹URL**ã¯https://localhost:11434ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã™ï¼ˆä»¥ä¸‹ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå‚ç…§ï¼‰ã€‚Ollamaã‚µãƒ¼ãƒ“ã‚¹æ¥ç¶šã®çŠ¶æ…‹ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã®æ¨ªã«ã‚ã‚‹**æ›´æ–°ãƒœã‚¿ãƒ³**ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚WebUIãŒOllamaã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ããªã„å ´åˆã€`WebUI could not connect to Ollama`ã¨ã„ã†ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚"
      }
    },
    {
      "segment_id": "7499a4e7",
      "source_content": "![Sage WebUI Ollama Setting Failure](https://llm-assets.readthedocs.io/en/latest/_images/open_webui_settings_0.png)",
      "source_content_hash": "b5dc32f513c7e33eddea38ed69c361697f605cf7bda65677d9ed859e7c0713ac",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "![Sage WebUI Ollamaè¨­å®šå¤±æ•—](https://llm-assets.readthedocs.io/en/latest/_images/open_webui_settings_0.png)"
      }
    },
    {
      "segment_id": "517051a7",
      "source_content": "If the connection is successful, you will see a message stating `Service Connection Verified`, as illustrated below.",
      "source_content_hash": "5f84737359d32653e97cc97b8136903257dfdb63b68788e04e86a1764ddc4ecd",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "æ¥ç¶šãŒæˆåŠŸã—ãŸå ´åˆã€ä»¥ä¸‹ã®ã‚ˆã†ã«`Service Connection Verified`ã¨ã„ã†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚"
      }
    },
    {
      "segment_id": "135468ee",
      "source_content": "![Sage WebUI Ollama Setting Success](https://llm-assets.readthedocs.io/en/latest/_images/open_webui_settings.png)",
      "source_content_hash": "65bf627637fb2c26141b23f3ad70a9400c1a19d5ad15ad922a3d7cc49512ffdf",
      "node_type": "paragraph",
      "translatable": true,
      "translations": {
        "ja": "![Sage WebUI Ollamaè¨­å®šæˆåŠŸ](https://llm-assets.readthedocs.io/en/latest/_images/open_webui_settings.png)"
      }
    },
    {
      "segment_id": "33a4805e",
      "source_content": ":::tip\nIf you want to use an Ollama server hosted at a different URL, simply update the **Ollama Base URL** to the new URL and press the **Refresh** button to re-confirm the connection to Ollama.\n:::",
      "source_content_hash": "2030606d1b7f11d2b09e661aeb43f0a2c93a7b40a2fa3ec8af0298258450d9fb",
      "node_type": "containerDirective",
      "translatable": true,
      "translations": {
        "ja": ":::tip\nåˆ¥ã®URLã§ãƒ›ã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹Ollamaã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ã—ãŸã„å ´åˆã¯ã€**Ollamaãƒ™ãƒ¼ã‚¹URL**ã‚’æ–°ã—ã„URLã«æ›´æ–°ã—ã€**æ›´æ–°**ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦Ollamaã¸ã®æ¥ç¶šã‚’å†ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n:::"
      }
    }
  ],
  "target_i18n_subpath": "docusaurus-plugin-content-docs/current/tutorials/integrations/ipex_llm.md",
  "last_updated_timestamp": "2025-06-06T09:21:13.805378+00:00",
  "schema_version": "1.0",
  "translated_versions": {
    "ja": "05a6adae94d46beeb9b2e32173a168103f2934541a47bac4bfe4f3da541b681a"
  }
}