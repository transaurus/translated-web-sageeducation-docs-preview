---
sidebar_position: 6
title: "📝 Evaluation"
---

## モデル評価が必要な理由

**アレックス**は中規模企業の機械学習エンジニアです。世の中にはGPTやLLaMAなど多くのAIモデルがありますが、どれが自分の仕事に最適か？ 公式のリーダーボードだけでは判断できません。モデルの性能は文脈によって異なり、評価データセットで訓練されたモデルもあるからです（ずるいですね）。また、モデルによって文章の「感じ」が違うこともあります。

そこでSage WebUIの出番です。アレックスとチームは実際のニーズに基づいてモデルを簡単に評価できます。複雑な計算も不要。ただ会話しながらサムズアップ/ダウンするだけです。

### 要約

- **評価の重要性**: 多くのモデルがあるが、特定のニーズに合うとは限らない。一般のリーダーボードは必ずしも信頼できない
- **解決策**: Sage WebUIの組み込み評価システム。レスポンスをサムズアップ/ダウンで評価
- **内部処理**: 評価が個人用リーダーボードを更新。評価されたチャットのスナップショットは将来のモデル改善に使用
- **評価方法**:
  - **アリーナモード**: ランダムにモデルを選択して比較
  - **通常インタラクション**: 普段通りチャットしつつレスポンスを評価

---

### 公開評価だけでは不十分な理由

- 公開リーダーボードは**あなたの**特定のユースケースに最適化されていない
- 評価データセットで訓練されたモデルがあり、結果の公平性に影響
- 全体的に性能が良くても、コミュニケーションスタイルが求めている「雰囲気」と合わない場合がある

### 解決策: Sage WebUIによるパーソナライズド評価

Sage WebUIの評価機能を使えば、チームの特定のニーズに最適なモデルを発見できます。すべてモデルと対話しながら行えます。

仕組みはシンプル！

- **チャット中**、レスポンスが良ければサムズアップ、悪ければサムズダウン。**兄弟メッセージ**（再生成されたレスポンスや並列比較中のメッセージなど）がある場合、評価は**個人用リーダーボード**に反映
- **リーダーボード**は管理セクションから簡単にアクセス可能。チームにとって性能の良いモデルを追跡

注目機能：**レスポンスを評価するたび**、システムは**会話のスナップショット**を保存。これは将来のモデル改善やトレーニングに使用されます（現在開発中）

---

### AIモデルを評価する2つの方法

Sage WebUIでは2つのシンプルな方法でAIモデルを評価できます。

### **1. アリーナモード**

**アリーナモード**は利用可能なモデルプールからランダムに選択し、公平で偏りのない評価を実現します。手動比較の潜在的問題（**生態学的妥当性** - 無意識に特定モデルを優遇してしまうこと）を防ぎます。

使用方法:

- アリーナモードセレクターからモデルを選択
- 普段通り使用しますが、現在は「アリーナモード」中

リーダーボードに反映させるには**兄弟メッセージ**が必要です。兄弟メッセージとは？ 同じクエリに対する代替レスポンス（メッセージの再生成や複数モデルによる並列生成など）のこと。これにより**直接比較**が可能になります。

- **評価のコツ**: 一方をサムズアップすると、他方は自動的にサムズダウンされます。本当に優れていると考えるメッセージのみをアップ投票しましょう
- 評価後、リーダーボードでモデルの順位を確認可能

アリーナモードのインターフェース動作イメージはこちら:

![Arena Model Example](/images/evaluation/arena.png)

さらに詳しく知りたいですか？[**Chatbot Arena**](https://lmarena.ai/)スタイルのセットアップも再現可能です！

![Chatbot Arena Example](/images/evaluation/arena-many.png)

### **2. 通常のインタラクション**

「アリーナモード」に切り替えたくない場合でも問題ありません。Sage WebUIを通常通り使用し、日常的な操作と同様にAIモデルの応答を評価できます。気が向いたときにモデルの応答に対して👍/👎を付けるだけです。ただし、**フィードバックをリーダーボードのランキングに反映させたい場合**、**モデルを切り替えて別のモデルとインタラクションする必要があります**。これにより、比較対象となる**兄弟レスポンス**が存在するようになります——異なるモデル間の比較のみがランキングに影響を与えます。

例えば、通常のインタラクション中に評価する方法は以下の通りです：

![通常のモデル評価インターフェース](/images/evaluation/normal.png)

また、アリーナと同様のマルチモデル比較を設定する例はこちらです：

![マルチモデル比較](/images/evaluation/normal-many.png)

---

## リーダーボード

評価後、Admin Panelの**リーダーボード**を確認してください。ここでは、**Eloレーティングシステム**（チェスのランキングのようなもの！）を使用してモデルのパフォーマンスが視覚的に表示され、評価中に真に優れたモデルが一目でわかります。

リーダーボードのレイアウト例は以下の通りです：

![リーダーボードの例](/images/evaluation/leaderboard.png)

### トピック別再ランキング

チャットを評価する際、**トピックでタグ付け**してより詳細な洞察を得ることができます。これは**カスタマーサポート、クリエイティブライティング、テクニカルサポート**など、異なるドメインで作業している場合に特に有用です。

#### 自動タグ付け

Sage WebUIは会話のトピックに基づいて**自動的にチャットにタグを付けようとします**。ただし、使用するモデルによっては、自動タグ付け機能が**時々失敗したり**会話を誤解釈したりする可能性があります。このような場合は、フィードバックの正確性を確保するために**手動でチャットにタグを付ける**ことがベストプラクティスです。

- **手動タグ付け方法**: 応答を評価する際、会話のコンテキストに基づいて独自のタグを追加するオプションがあります。

このステップを省略しないでください！タグ付けは非常に強力な機能で、**特定のトピックに基づいてモデルを再ランク付け**できます。例えば、テクニカルサポートの質問に対する回答と一般的な顧客問い合わせに対して、どのモデルが最も優れているかを確認したい場合などです。

再ランキングの例は以下の通りです：

![トピック別再ランキングリーダーボード](/images/evaluation/leaderboard-reranked.png)

---

### 補足：モデルファインチューニングのためのチャットスナップショット

モデルの応答を評価するたびに、Sage WebUIは*そのチャットのスナップショットをキャプチャします*。これらのスナップショットは、最終的に**独自のモデルのファインチューニング**に使用できます——つまり、評価がAIの継続的な改善に貢献します。

*(この機能に関する最新情報にご期待ください、現在積極的に開発中です！)*

---

## まとめ

**要約すると**、Sage WebUIの評価システムには2つの明確な目的があります：

1. **モデルの簡単な比較**を支援すること。
2. 最終的に、個々のニーズに最も適したモデルを見つけること。

このシステムの核心は、AIモデルの評価を**シンプルで透明性があり、カスタマイズ可能**なものにすることです。アリーナモデルでも通常のチャットインタラクションでも、**特定のユースケースに最適なAIモデルを決定する完全なコントロールがあなたにあります**！

**常に**、すべてのデータは**あなたのインスタンス**上で安全に保持され、特に**コミュニティ共有を選択**しない限り、何も共有されません。あなたのプライバシーとデータの自律性は常に最優先されます。